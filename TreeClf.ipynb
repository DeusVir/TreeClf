{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yZIZCxxcteBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bc1f99-86b9-459a-d73f-396969ce82c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "col_0 > -0.1\n",
            " Leaf: 0.8235294117647058\n",
            " Leaf: 0.07692307692307693\n",
            "75.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "class MyTreeClf():\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20, bins = None, criterion = 'entropy'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_leafs = max_leafs\n",
        "        self.leafs_cnt = 1\n",
        "        self.tree = {}\n",
        "        self.bins = bins\n",
        "        self.feature_bins = {}\n",
        "        self.criterion = criterion\n",
        "        self.fi = {}\n",
        "        self.N=None\n",
        "\n",
        "        if self.bins is not None:\n",
        "            for col in X.columns:\n",
        "                unique_values = sorted(X[col].unique())\n",
        "                if len(unique_values) - 1 <= self.bins:\n",
        "                    self.feature_bins[col] = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)]\n",
        "                else:\n",
        "                    hist, bin_edges = np.histogram(X[col], bins=self.bins)\n",
        "                    self.feature_bins[col] = bin_edges[:-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"MyTreeClf class: \" + \", \".join(('{}={}'.format(item, self.__dict__[item]) for item in self.__dict__))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MyTreeClf class: \" + \", \".join(('{}={}'.format(item, self.__dict__[item]) for item in self.__dict__))\n",
        "\n",
        "    def get_best_split(self, X, y):\n",
        "        best_score = -1\n",
        "        best_col_name = None\n",
        "        best_split_value = None\n",
        "\n",
        "        for col_name in X.columns:\n",
        "            split_values = self.feature_bins.get(col_name, [])\n",
        "            if not np.array(split_values).any():\n",
        "                unique_values = sorted(X[col_name].unique())\n",
        "                split_values = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)]\n",
        "\n",
        "            for split_value in split_values:\n",
        "                left_indices = X[col_name] <= split_value\n",
        "                right_indices = X[col_name] > split_value\n",
        "\n",
        "                if self.criterion == 'entropy':\n",
        "                    left_entropy = self._entropy(y[left_indices])\n",
        "                    right_entropy = self._entropy(y[right_indices])\n",
        "                    weighted_entropy = (len(y[left_indices]) / len(y)) * left_entropy + (len(y[right_indices]) / len(y)) * right_entropy\n",
        "                    score = self._entropy(y) - weighted_entropy\n",
        "                elif self.criterion == 'gini':\n",
        "                    left_gini = self._gini(y[left_indices])\n",
        "                    right_gini = self._gini(y[right_indices])\n",
        "                    weighted_gini = (len(y[left_indices]) / len(y)) * left_gini + (len(y[right_indices]) / len(y)) * right_gini\n",
        "                    score = self._gini(y) - weighted_gini\n",
        "                else:\n",
        "                    raise ValueError(\"Invalid criterion. Choose 'entropy' or 'gini'.\")\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_col_name = col_name\n",
        "                    best_split_value = split_value\n",
        "\n",
        "        return best_col_name, best_split_value, best_score\n",
        "\n",
        "    def _entropy(self, y):\n",
        "          _, counts = np.unique(y, return_counts=True)\n",
        "          probabilities = counts / len(y)\n",
        "          entropy = -np.sum(probabilities * np.log2(probabilities, where=(probabilities != 0)))\n",
        "          return entropy\n",
        "\n",
        "    def _gini(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        gini = 1 - np.sum(probabilities ** 2)\n",
        "        return gini\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.N=len(y)\n",
        "        self.leafs_cnt = 1\n",
        "        self.fi = {col: 0 for col in X.columns}\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        proportion_ones = len(y[y == 1]) / len(y) if len(y) else 0\n",
        "        if proportion_ones == 1 or proportion_ones == 0 or depth >= self.max_depth or len(y) < self.min_samples_split or (self.leafs_cnt > 1 and self.leafs_cnt >= self.max_leafs):\n",
        "            return {'leaf': True, 'value': proportion_ones}\n",
        "\n",
        "        best_col, best_split, ig = self.get_best_split(X, y)\n",
        "        left_indices = X[best_col] <= best_split\n",
        "        right_indices = X[best_col] > best_split\n",
        "        self.leafs_cnt += 1\n",
        "\n",
        "        N = self.N\n",
        "        Np = len(y)\n",
        "        Nl = len(y[left_indices])\n",
        "        Nr = len(y[right_indices])\n",
        "        I = self._entropy(y) if self.criterion == 'entropy' else self._gini(y)\n",
        "        Il = self._entropy(y[left_indices]) if self.criterion == 'entropy' else self._gini(y[left_indices])\n",
        "        Ir = self._entropy(y[right_indices]) if self.criterion == 'entropy' else self._gini(y[right_indices])\n",
        "\n",
        "\n",
        "        self.fi[best_col] += (Np / N) * (I - (Nl / Np) * Il - (Nr / Np) * Ir)\n",
        "\n",
        "        return {\n",
        "            'col': best_col,\n",
        "            'split': best_split,\n",
        "            'left': self._build_tree(X[left_indices], y[left_indices], depth + 1),\n",
        "            'right': self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        }\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return [self._predict_proba_single(row) for _, row in X.iterrows()]\n",
        "\n",
        "    def _predict_proba_single(self, row):\n",
        "        node = self.tree\n",
        "        while 'leaf' not in list(node.keys()):\n",
        "            if row[node['col']] <= node['split']:\n",
        "                node = node['left']\n",
        "            else:\n",
        "                node = node['right']\n",
        "        return node['value']\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [1 if p > 0.5 else 0 for p in self.predict_proba(X)]\n",
        "\n",
        "\n",
        "    def print_tree(self, node=None, depth=0):\n",
        "        if node is None:\n",
        "            node = self.tree\n",
        "\n",
        "        if 'leaf' in node:\n",
        "            print(f\"{' ' * depth}Leaf: {node['value']}\")\n",
        "        else:\n",
        "            print(f\"{' ' * depth}{node['col']} > {node['split']}\")\n",
        "            self.print_tree(node['left'], depth + 1)\n",
        "            self.print_tree(node['right'], depth + 1)\n",
        "\n",
        "    def get_feature_importances(self):\n",
        "        return self.fi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sample data\n",
        "X, y = make_classification(n_samples=150, n_features=5, n_informative=3, random_state=42)\n",
        "X = pd.DataFrame(X).round(2)\n",
        "y = pd.Series(y)\n",
        "X.columns = [f'col_{col}' for col in X.columns]\n",
        "test = X.sample(20, random_state=42)\n",
        "\n",
        "tree = MyTreeClf(\n",
        "    max_depth = 3,\n",
        "    min_samples_split = 2,\n",
        "    max_leafs = 1\n",
        ")\n",
        "tree.fit(X, y)\n",
        "\n",
        "tree.print_tree()\n",
        "\n",
        "print(np.sum(tree.predict_proba(X)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vv9ces65b9Oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}